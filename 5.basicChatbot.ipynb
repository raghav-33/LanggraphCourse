{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80ebaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from typing import TypedDict , Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import BaseMessage ,SystemMessage,HumanMessage,AIMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb73699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Define\n",
    "from langgraph.graph.message import add_messages\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages ]  # Base Messages : all types of messages is derived from it , it indicate here any type of message can be present either human , AI or system message\n",
    "\n",
    "# add_messages: is a reducer function like operator.add in previous code.\n",
    "# Why ?\n",
    "# Because nature of state is when it get new value its old value (message) is deleted (state Update on new messages).\n",
    "# therefore add_messages (reducer function) is  used to maintain conversational history, not forget previous messages or states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM define\n",
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9815ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ChatState):\n",
    "    # Take User Query From state\n",
    "    messages = state['messages']\n",
    "\n",
    "    # Send to llm\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Response store state\n",
    "    return {'messages' : [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697ea0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Define\n",
    "graph = StateGraph(ChatState)\n",
    "\n",
    "# nodes add\n",
    "graph.add_node('chat_node', chat_node)\n",
    "\n",
    "# Edge add\n",
    "graph.add_edge(START,'chat_node')\n",
    "graph.add_edge('chat_node',END)\n",
    "\n",
    "# Compile\n",
    "chatbot = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23110df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Graph (static | console implemnatation cheking or testing)\n",
    "initial_state = {\n",
    "    'messages' : [HumanMessage(content = \"What is Capital of India\")]\n",
    "}\n",
    "\n",
    "chatbot.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bea7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot Logic\n",
    "while True:\n",
    "    user_message = input('Type Here')\n",
    "\n",
    "    print(\"User:\" , user_message)\n",
    "\n",
    "    if user_message.strip().lower() in ['exit', 'quit' , 'bye']:\n",
    "        break\n",
    "    response= chatbot.invoke({'messages': [HumanMessage(content=user_message)]})\n",
    "    \n",
    "    print('AI:', response['messages'[-1].content])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70238276",
   "metadata": {},
   "source": [
    "Normally\n",
    "when we reach at end of workflow , our state get erased . every time when we newly invoke (workflow) ,newly state start\n",
    "\n",
    "\n",
    "But in Persistence\n",
    "at end of workflow state not get erased and we store it in various places like Databases , memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44087528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from typing import TypedDict , Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import BaseMessage ,SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "# Persistance Import\n",
    "from langgraph.checkpoint.memory import MemorySaver  # kind of memory in langraph which store things in RAM\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15802dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Define\n",
    "from langgraph.graph.message import add_messages\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages ]  # Base Messages : all types of messages is derived from it , it indicate here any type of message can be present either human , AI or system message\n",
    "\n",
    "# add_messages: is a reducer function like operator.add in previous code.\n",
    "# Why ?\n",
    "# Because nature of state is when it get new value its old value (message) is deleted .\n",
    "# therefore add_messages (reducer function) is  used to maintain conversational history, not forget previous messages or states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a68715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM define\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b8364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ChatState):\n",
    "    # Take User Query From state\n",
    "    messages = state['messages']\n",
    "\n",
    "    # Send to llm\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Response store state\n",
    "    return {'messages' : [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d8140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Define\n",
    "\n",
    "checkpointer = MemorySaver()  # Persistance : memory\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "\n",
    "# nodes add\n",
    "graph.add_node('chat_node', chat_node)\n",
    "\n",
    "# Edge add\n",
    "graph.add_edge(START,'chat_node')\n",
    "graph.add_edge('chat_node',END)\n",
    "\n",
    "# Compile\n",
    "chatbot = graph.compile(checkpointer=checkpointer) # memory get implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Graph\n",
    "initial_state = {\n",
    "    'messages' : [HumanMessage(content = \"What is Capital of India\")]\n",
    "}\n",
    "\n",
    "chatbot.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot Logic\n",
    "\n",
    "''' \n",
    "What are Threads ?  \n",
    "A thread is the smallest unit of execution inside a process.\n",
    "Multiple threads can run concurrently within the same program. \n",
    "\n",
    "Think of it like:\n",
    "    Process → Python program\n",
    "    Thread → tasks running inside that program.   \n",
    "\n",
    "\n",
    "Example:\n",
    "    One thread handles user input\n",
    "    Another thread handles background computation\n",
    "\n",
    "\n",
    "What is Thread Id ?\n",
    "A thread ID is a unique identifier assigned to each thread by the operating system.\n",
    "It helps:\n",
    "    Identify which thread is running\n",
    "    Debug concurrency issues\n",
    "    Track logs per thread\n",
    "\n",
    "'''\n",
    "\n",
    "thread_id= '1' # Basically at same time chatbot is used by many peoples . so thread id is chatbot kis people sai kya baat kr rha hai.\n",
    "while True:\n",
    "    user_message = input('Type Here')\n",
    "\n",
    "    print(\"User:\" , user_message)\n",
    "\n",
    "    if user_message.strip().lower() in ['exit', 'quit' , 'bye']:\n",
    "        break\n",
    "\n",
    "    config = {'configurable': {'thread_id':thread_id}}  # This configuration tells LangGraph which conversation thread (session) the graph execution belongs to. (ii) In LangGraph, config = {\"configurable\": {\"thread_id\": thread_id}} is used to identify and persist a conversation thread so the graph can maintain state across multiple executions.\n",
    "    '''  This tells LangGraph:\n",
    "        “Use this thread_id to store and retrieve conversation state”     \n",
    "         \n",
    "        Why needed:\n",
    "               Same chatbot used by multiple users\n",
    "               Each user must have separate memory\n",
    "\n",
    "        Without thread_id: ❌ Every message treated as a new conversation\n",
    "        With thread_id: ✅ Chatbot remembers previous messages\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    response= chatbot.invoke({'messages': [HumanMessage(content=user_message)]},config=config)\n",
    "    \n",
    "    print('AI:', response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f92d09",
   "metadata": {},
   "source": [
    "Why do we use thread_id = \"1\"?\n",
    "\n",
    "\"1\" is just a placeholder to represent a single conversation session.\n",
    "\n",
    "It means: (i) Only one user (ii) Only one conversation\n",
    "\n",
    "Memory will always be reused for that same session\n",
    "\n",
    "So this is okay for learning/demo, ❌ not correct for production.\n",
    "\n",
    "Why \"1\" is NOT correct in real applications ⚠️?\n",
    "\n",
    "| User   | thread_id |\n",
    "| ------ | --------- |\n",
    "| User A | \"1\"       |\n",
    "| User B | \"1\"       |\n",
    "| User C | \"1\"       |\n",
    "\n",
    "❌ Problem:\n",
    "\n",
    "All users share the same memory\n",
    "\n",
    "One user’s messages affect another’s answers\n",
    "\n",
    "Solution : UUID\n",
    "\n",
    "import uuid\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view all converstional History\n",
    "chatbot.get_state(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
