{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f2f881",
   "metadata": {},
   "source": [
    "                   START\n",
    "                    |\n",
    "                    |\n",
    "                    |\n",
    "                    |\n",
    "                GENERATE_JOKE\n",
    "                    |\n",
    "                    |\n",
    "                    |\n",
    "                    |\n",
    "                GENERATE EXPLANATION\n",
    "                    |\n",
    "                    |\n",
    "                    |\n",
    "                    |\n",
    "                   END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f3b702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.checkpoint.memory import InMemorySaver # for Store in RAM\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cf2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Define\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cc1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state Define\n",
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7167351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1 \n",
    "\n",
    "def generate_joke(state: JokeState):\n",
    "\n",
    "    prompt = f'generate a joke on the topic {state[\"topic\"]}'\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    return {'joke': response}\n",
    "\n",
    "# Function 2\n",
    "\n",
    "def generate_explanation(state: JokeState):\n",
    "\n",
    "    prompt = f'write an explanation for the joke - {state[\"joke\"]}'\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    return {'explanation': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc340fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Define\n",
    "graph = StateGraph(JokeState)\n",
    "\n",
    "#Node\n",
    "graph.add_node('generate_joke', generate_joke)\n",
    "graph.add_node('generate_explanation', generate_explanation)\n",
    "\n",
    "#Edges\n",
    "graph.add_edge(START, 'generate_joke')\n",
    "graph.add_edge('generate_joke', 'generate_explanation')\n",
    "graph.add_edge('generate_explanation', END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Compile the Graph\n",
    "workflow = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "workflow.invoke({'topic':'pizza'}, config=config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get Entire Store Things | History\n",
    "workflow.get_state(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62011e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get Intermediate states value (for every node)\n",
    "list(workflow.get_state_history(config1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868fbf9b",
   "metadata": {},
   "source": [
    "# Benfits of Persistance\n",
    "we can implement Short term memory in chatbot.\n",
    "\n",
    "fault tolerance. : resume chat exactly where crashes happen\n",
    "\n",
    "HITL implement (Human in the loop).\n",
    "\n",
    "Time Travel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4154f69",
   "metadata": {},
   "source": [
    "# FAULT TOLERANCE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e90251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa3fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the state\n",
    "class CrashState(TypedDict):\n",
    "    input: str\n",
    "    step1: str\n",
    "    step2: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35308174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define steps\n",
    "def step_1(state: CrashState) -> CrashState:\n",
    "    print(\"âœ… Step 1 executed\")\n",
    "    return {\"step1\": \"done\", \"input\": state[\"input\"]}\n",
    "\n",
    "def step_2(state: CrashState) -> CrashState:\n",
    "    print(\"â³ Step 2 hanging... now manually interrupt from the notebook toolbar (STOP button)\")\n",
    "    time.sleep(1000)  # Delay introduced | Simulate long-running hang\n",
    "    return {\"step2\": \"done\"}\n",
    "\n",
    "def step_3(state: CrashState) -> CrashState:\n",
    "    print(\"âœ… Step 3 executed\")\n",
    "    return {\"done\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the graph\n",
    "builder = StateGraph(CrashState)\n",
    "builder.add_node(\"step_1\", step_1)\n",
    "builder.add_node(\"step_2\", step_2)\n",
    "builder.add_node(\"step_3\", step_3)\n",
    "\n",
    "\n",
    "builder.add_edge(\"step_1\", \"step_2\")\n",
    "builder.add_edge(\"step_2\", \"step_3\")\n",
    "builder.add_edge(\"step_3\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    print(\"â–¶ï¸ Running graph: Please manually interrupt during Step 2...\")\n",
    "    graph.invoke({\"input\": \"start\"}, config={\"configurable\": {\"thread_id\": 'thread-1'}})\n",
    "except KeyboardInterrupt:\n",
    "    print(\"âŒ Kernel manually interrupted (crash simulated).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4436525",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "â–¶ï¸ Running graph: Please manually interrupt during Step 2...\n",
    "\n",
    "âœ… Step 1 executed\n",
    "\n",
    "â³ Step 2 hanging... now manually interrupt from the notebook toolbar (STOP button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff63fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Re-run to show fault-tolerant resume\n",
    "print(\"\\nðŸ” Re-running the graph to demonstrate fault tolerance...\")\n",
    "final_state = graph.invoke(None, config={\"configurable\": {\"thread_id\": 'thread-1'}})  # NONE    \n",
    "# { None } Indicate : resume execution where it stop and providing same thread id of it.\n",
    "print(\"\\nâœ… Final State:\", final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653f956",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "\n",
    "âœ… Step 2 hanging... now manually interrupt from the notebook toolbar (STOP button)\n",
    "\n",
    "âœ… Step 3 executed\n",
    "\n",
    "ðŸ“Œ Final State:\n",
    "\n",
    "{\n",
    "  \n",
    "  'input': 'start',\n",
    "  \n",
    "  'step1': 'done',\n",
    "  \n",
    "  'step2': 'done',\n",
    "  \n",
    "  'step3': 'done'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(graph.get_state_history({\"configurable\": {\"thread_id\": 'thread-1'}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259cd75",
   "metadata": {},
   "source": [
    "# HUMAN IN LOOP\n",
    "\n",
    "(see by click on edit button)\n",
    "\n",
    "topic --------> Linkedin Post -------> API\n",
    "                                 ^\n",
    "                                 |\n",
    "                                 |\n",
    "                                 |\n",
    "persistance   ..............   Interrupt  ........... Here for\n",
    " \n",
    "human interaction click the button to post on linkedin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980bf5b",
   "metadata": {},
   "source": [
    "# Time Travel\n",
    "\n",
    "After the Complete Execution we can go at any place|Node|checkpoint to see working after that or Replay the execution\n",
    "\n",
    "What benfit? \n",
    "\n",
    "Debugging ( we can see where at which node | checkpoint error is occuring)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8664be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_state({\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1f06cc6e-7232-6cb1-8000-f71609e6cec5\"}})\n",
    "# we also give (checkpoint id) at where we want to resume execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72f01a",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "\n",
    "StateSnapshot(values={'topic': 'pizza'}, next=('generate_joke',), config={'configurable': {'thread_id': '1', 'checkpoint_id': '1f06cc6e-7232-6cb1-8000-f71609e6cec5'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}, 'thread_id': '1'}, created_at='2025-07-29T21:56:38.565188+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f06cc6e-7230-65a8-bfff-0a96c2fc4e11'}}, tasks=(PregelTask(id='dcd96e38-1f32-5ed6-9f44-fa2b22c193f0', name='generate_joke', path=('__pregel_pull', 'generate_joke'), error=None, interrupts=(), state=None, result={'joke': 'Why did the pizza go to the doctor? Because it was feeling a little saucy!'}),), interrupts=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e33dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.invoke(None, {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1f06cc6e-7232-6cb1-8000-f71609e6cec5\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93249326",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "\n",
    "{'topic': 'pizza',\n",
    " 'joke': 'Why did the mushroom go to the pizza party? Because he was a fungi and everyone wanted a pizza him!',\n",
    " 'explanation': 'This joke plays on the word \"fun guy\" (fungi) which sounds like \"fungi,\" a type of mushroom. The play on words is that the mushroom went to the pizza party because he was a \"fun guy\" and people wanted to \"pizza\" (see) him. The joke is a pun that combines the idea of mushrooms being fungi with the concept of being a fun person at a party."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53fba0",
   "metadata": {},
   "source": [
    "# Updating state\n",
    "\n",
    "we can also change the value of state at particular checkpoint \n",
    "\n",
    "like in previous conversation joke was on {Pizza}\n",
    "\n",
    "now we can change topic (update state) : generate a joke on apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.update_state({\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1f06cc6e-7232-6cb1-8000-f71609e6cec5\", \"checkpoint_ns\": \"\"}}, {'topic':'samosa'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.invoke(None, {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1f06cc72-ca16-6359-8001-7eea05e07dd2\"}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
